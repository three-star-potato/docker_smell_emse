import json
import requests
import random
import re
from tqdm import tqdm
import fnmatch
import os
import sys
import signal
import time
from datetime import datetime

def save_time_records(time_records, filename, mode='w'):
    """ä¿å­˜æ—¶é—´è®°å½•åˆ°æ–‡ä»¶"""
    if not time_records:
        return
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(os.path.dirname(filename), exist_ok=True)
    
    # ç¡®å®šæ–‡ä»¶æ ¼å¼
    if filename.endswith('.json'):
        with open(filename, mode, encoding='utf-8') as f:
            if mode == 'a' and os.path.exists(filename) and os.path.getsize(filename) > 0:
                # è¯»å–ç°æœ‰æ•°æ®å¹¶è¿½åŠ 
                try:
                    f.seek(0)
                    existing_data = json.load(f)
                    existing_data.extend(time_records)
                    f.seek(0)
                    f.truncate()
                    json.dump(existing_data, f, indent=2, ensure_ascii=False)
                except (json.JSONDecodeError, Exception) as e:
                    print(f"Error reading existing JSON file: {e}, creating new file")
                    json.dump(time_records, f, indent=2, ensure_ascii=False)
            else:
                json.dump(time_records, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… Time records saved: {filename}")

def generate_summary_report(time_log_file, output_file=None):
    """ç”Ÿæˆä¿®å¤æ—¶é—´æ‘˜è¦æŠ¥å‘Š"""
    if not os.path.exists(time_log_file):
        print(f"Time log file not found: {time_log_file}")
        return
    
    if time_log_file.endswith('.json'):
        with open(time_log_file, 'r', encoding='utf-8') as f:
            records = json.load(f)
    else:
        records = []
    
    if not records:
        print("No records found in time log")
        return
    
    # åˆ†ææ•°æ®
    successful_repairs = [r for r in records if r.get('status') == 'success']
    failed_repairs = [r for r in records if r.get('status') in ['error', 'timeout']]
    
    summary = {
        'total_files': len(records),
        'successful_repairs': len(successful_repairs),
        'failed_repairs': len(failed_repairs),
        'avg_repair_time': round(sum(r.get('repair_time_seconds', 0) for r in records) / len(records), 2) if records else 0,
        'total_processing_time': round(sum(r.get('repair_time_seconds', 0) for r in records), 2),
        'timestamp': datetime.now().isoformat()
    }
    
    # æ‰“å°æ‘˜è¦
    print("\n" + "="*50)
    print("ä¿®å¤æ—¶é—´æ‘˜è¦æŠ¥å‘Š")
    print("="*50)
    print(f"æ€»å¤„ç†æ–‡ä»¶æ•°: {summary['total_files']}")
    print(f"æˆåŠŸä¿®å¤: {summary['successful_repairs']}")
    print(f"ä¿®å¤å¤±è´¥: {summary['failed_repairs']}")
    print(f"å¹³å‡ä¿®å¤æ—¶é—´: {summary['avg_repair_time']}ç§’")
    print(f"æ€»å¤„ç†æ—¶é—´: {summary['total_processing_time']}ç§’")
    
    # ä¿å­˜æ‘˜è¦æŠ¥å‘Š
    if output_file:
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        print(f"æ‘˜è¦æŠ¥å‘Šå·²ä¿å­˜è‡³: {output_file}")
    
    return summary

def send_message_and_get_response(message, model_name, no_think=False, use_openai_api=False, use_cpu=False):
    """å‘é€æ¶ˆæ¯å¹¶è·å–å“åº”ï¼Œæ”¯æŒOpenAI APIå’Œæœ¬åœ°Ollama API"""
    if use_openai_api:
        return _call_openai_api(message, model_name)
    else:
        return _call_ollama_api(message, model_name, no_think, use_cpu)

def _call_openai_api(message, model_name):
    """è°ƒç”¨OpenAIå…¼å®¹APIï¼ˆç™¾ç‚¼ï¼‰"""
    try:
        from openai import OpenAI
        
        client = OpenAI(
            api_key="fake_api",
            base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
        )
        
        messages = [{"role": "user", "content": message}]
        
        # è®¾ç½®è¶…æ—¶
        def timeout_handler(signum, frame):
            raise TimeoutError("Request timed out")
        
        signal.signal(signal.SIGALRM, timeout_handler)
        signal.alarm(180)
        
        try:
            completion = client.chat.completions.create(
                model=model_name,
                messages=messages,
                stream=False,
                temperature=0.3,
                max_tokens=4096
            )
            signal.alarm(0)  # é‡ç½®è¶…æ—¶
            
            message_content = completion.choices[0].message.content
            
            # æå–Dockerfileå†…å®¹
            dockerfile_pattern = re.compile(r'```dockerfile(.*?)```', re.DOTALL | re.IGNORECASE)
            match = dockerfile_pattern.search(message_content)
            if match:
                dockerfile_content = match.group(1).strip()
                return dockerfile_content
            else:
                print("No Dockerfile found in the response")
                return None
                
        except TimeoutError:
            print("Request timed out after 180 seconds")
            return None
        except Exception as e:
            print(f"OpenAI APIè°ƒç”¨é”™è¯¯: {str(e)}")
            return None
            
    except ImportError:
        print("OpenAIåº“æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install openai")
        return None
    except Exception as e:
        print(f"åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯é”™è¯¯: {str(e)}")
        return None

def _call_ollama_api(message, model_name, no_think=False, use_cpu=False):
    """è°ƒç”¨æœ¬åœ°Ollama API"""
    url = "http://localhost:11434/api/chat"
    
    # æ¨¡å‹å·®å¼‚åŒ–æ§åˆ¶
    if no_think:
        if "qwen3" in model_name.lower():
            message = f"/no_think\n\n{message}"
    
    messages = [{"role": "user", "content": message}]
    
    payload = {
        "model": model_name,
        "messages": messages,
        "stream": False,
        "options": {
            "temperature": 0.3,
            "num_predict": 4096
        }
    }
    
    # æ·»åŠ CPUè¿è¡Œé€‰é¡¹
    if use_cpu:
        payload["options"]["num_gpu"] = 0  # å¼ºåˆ¶ä½¿ç”¨CPU
        print("ğŸ”§ ä½¿ç”¨CPUæ¨¡å¼è¿è¡Œæ¨¡å‹")

    try:
        # è®¾ç½®è¶…æ—¶
        def timeout_handler(signum, frame):
            raise requests.exceptions.Timeout("Request timed out")

        signal.signal(signal.SIGALRM, timeout_handler)
        signal.alarm(180)

        response = requests.post(url, json=payload)
        signal.alarm(0)  # é‡ç½®è¶…æ—¶

        if response.status_code == 200:
            result = response.json()
            message_content = result['message']['content']
            
            # æå–Dockerfileå†…å®¹
            dockerfile_pattern = re.compile(r'```dockerfile(.*?)```', re.DOTALL | re.IGNORECASE)
            match = dockerfile_pattern.search(message_content)
            if match:
                dockerfile_content = match.group(1).strip()
                return dockerfile_content
            else:
                print("No Dockerfile found in the response")
                return None
        else:
            print(f"APIè¿”å›é”™è¯¯çŠ¶æ€ç : {response.status_code}")
            return None

    except requests.exceptions.Timeout:
        print("Request timed out after 180 seconds")
        return None
    except requests.exceptions.RequestException as e:
        print("Error:", str(e))
        return None
    except Exception as e:
        print("Unexpected error:", str(e))
        return None

def process_dockerfiles(json_path, root_folder, mode_name, mode_dir, time_log_file=None, no_think=False, use_openai_api=False, use_cpu=False):
    """å¤„ç†Dockerfileså¹¶è®°å½•æ—¶é—´"""
    if not os.path.exists(mode_dir):
        os.makedirs(mode_dir)
    
    # æ—¶é—´è®°å½•æ•°æ®ç»“æ„
    time_records = []
    
    # Read data from the specified JSON file
    with open(json_path, 'r', encoding='utf-8') as file:
        data_json = json.load(file)
    
    # Iterate over each Dockerfile, read its content, modify it, and save to a new file
    for dockerfile in tqdm(sorted(data_json, key=lambda x: x['dockerfile_path'])):
        dockerfile_path = dockerfile["dockerfile_path"]
        issues = dockerfile["issues"]
        
        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()
        
        with open(dockerfile_path, 'r', encoding='utf-8') as file:
            original_content = file.read()
        
        modified_filepath = dockerfile_path.replace(root_folder, mode_dir)
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(modified_filepath), exist_ok=True)
        
        if not issues:
            with open(modified_filepath, 'w', encoding='utf-8') as file:
                file.write(original_content)
            print(f"{modified_filepath} Skipping with perfect.")
            
            # è®°å½•è·³è¿‡ä¿¡æ¯
            end_time = time.time()
            repair_time = end_time - start_time
            time_record = {
                'dockerfile': dockerfile_path,
                'repaired_file': modified_filepath,
                'repair_time_seconds': round(repair_time, 2),
                'status': 'skipped',
                'reason': 'no_issues',
                'timestamp': datetime.now().isoformat()
            }
            time_records.append(time_record)
            continue
        
        if os.path.exists(modified_filepath):
            print(f"Modified Dockerfile '{modified_filepath}' already exists. Skipping.")
            
            # è®°å½•è·³è¿‡ä¿¡æ¯
            end_time = time.time()
            repair_time = end_time - start_time
            time_record = {
                'dockerfile': dockerfile_path,
                'repaired_file': modified_filepath,
                'repair_time_seconds': round(repair_time, 2),
                'status': 'skipped',
                'reason': 'already_exists',
                'timestamp': datetime.now().isoformat()
            }
            time_records.append(time_record)
            continue
        
        dockerfile_issue_str = "\n".join(dockerfile["issues"])

        # æ„é€ ä¸¥æ ¼prompt
        prompt = (
            f"Original Dockerfile:\n```dockerfile\n{original_content}\n```\n\n"
            f"Smells need to fix:\n{dockerfile_issue_str}\n\n"
            "Return ONLY the modified Dockerfile that:\n"
            "1. Is directly buildable with `docker build`\n"
            "2. Preserves all original functionality\n"
            "3. NO new features added\n\n"
            "4. CRITICAL: ALL package versions MUST be preserved exactly as in original (apt-get, apk, yum, etc.)\n"
                "- If original has versions, keep them unchanged\n"
                "- If original has NO versions, do NOT add versions\n"
            "5. Format:\n```dockerfile\n...\n```"
        )

        modified_content = send_message_and_get_response(prompt, mode_name, no_think, use_openai_api, use_cpu)
        
        # è®°å½•ç»“æŸæ—¶é—´
        end_time = time.time()
        repair_time = end_time - start_time
        
        if modified_content:
            with open(modified_filepath, 'w', encoding='utf-8') as file:
                file.write(modified_content)
            
            # è®°å½•æˆåŠŸä¿¡æ¯
            time_record = {
                'dockerfile': dockerfile_path,
                'repaired_file': modified_filepath,
                'repair_time_seconds': round(repair_time, 2),
                'status': 'success',
                'timestamp': datetime.now().isoformat(),
                'model': mode_name,
                'no_think': no_think,
                'api_type': 'openai' if use_openai_api else 'ollama',
                'use_cpu': use_cpu
            }
            time_records.append(time_record)
            
            print(f"âœ… LLM repair executed successfully in {repair_time:.2f}s: {dockerfile_path}")
        else:
            print(f"Failed to modify Dockerfile '{dockerfile_path}'. Saved original as '{modified_filepath}'")
            with open(modified_filepath, 'w', encoding='utf-8') as file:
                file.write(original_content)
            
            # è®°å½•å¤±è´¥ä¿¡æ¯
            time_record = {
                'dockerfile': dockerfile_path,
                'repaired_file': modified_filepath,
                'repair_time_seconds': round(repair_time, 2),
                'status': 'error',
                'reason': 'llm_failed',
                'timestamp': datetime.now().isoformat(),
                'model': mode_name,
                'no_think': no_think,
                'api_type': 'openai' if use_openai_api else 'ollama',
                'use_cpu': use_cpu
            }
            time_records.append(time_record)
    
    # ä¿å­˜æ—¶é—´è®°å½•
    if time_log_file:
        save_time_records(time_records, time_log_file)
    
    print("All Dockerfiles processed.")
    return time_records

def remove_comments_in_lines(folder_path):
    """ç§»é™¤Dockerfileä¸­çš„æ³¨é‡Š"""
    # éå†æŒ‡å®šæ–‡ä»¶å¤¹ä¸‹çš„æ‰€æœ‰æ–‡ä»¶
    for filename in os.listdir(folder_path):
        filepath = os.path.join(folder_path, filename)
        # åªå¤„ç†ä»¥ Dockerfile å¼€å¤´çš„æ–‡ä»¶
       
        print(f"å¤„ç†æ–‡ä»¶: {filename}")
            # è¯»å–æ–‡ä»¶å†…å®¹
        with open(filepath, 'r') as f:
            lines = f.readlines()
            
            # å¤„ç†æ–‡ä»¶å†…å®¹ï¼Œå»é™¤æ¯è¡Œå†…çš„æ³¨é‡Š
        new_lines = []
        for line in lines:
            # å»é™¤è¡Œå°¾çš„ç©ºç™½å­—ç¬¦
            line = line.rstrip()
                # æŸ¥æ‰¾æ³¨é‡Šç¬¦å· '#' çš„ä½ç½®
            comment_index = line.find('#')
            if comment_index != -1:
                line = line[:comment_index].rstrip()  # å»é™¤æ³¨é‡Šéƒ¨åˆ†åçš„å†…å®¹
            new_lines.append(line + '\n')  # æ·»åŠ æ¢è¡Œç¬¦ä¿æŒåŸæœ‰æ ¼å¼
            
            # å°†å¤„ç†åçš„å†…å®¹å†™å›æ–‡ä»¶
        with open(filepath, 'w') as f:
            f.writelines(new_lines)
            
    print(f"å·²å®Œæˆ: {folder_path}")

def main():
    if len(sys.argv) < 5:
        print("Usage: python your_script.py json_path root_folder mode_name mode_dir [time_log_dir] [--no-think] [--use-openai-api] [--use-cpu]")
        print("\nå‚æ•°è¯´æ˜:")
        print("  json_path: JSONæ–‡ä»¶è·¯å¾„")
        print("  root_folder: åŸå§‹Dockerfileæ ¹ç›®å½•")
        print("  mode_name: æ¨¡å‹åç§°")
        print("  mode_dir: è¾“å‡ºç›®å½•")
        print("  time_log_dir: æ—¶é—´è®°å½•ç›®å½•ï¼ˆå¯é€‰ï¼‰")
        print("  --no-think: å¯ç”¨æ— æ€è€ƒæ¨¡å¼ï¼ˆä»…å¯¹Qwenæœ‰æ•ˆï¼‰")
        print("  --use-openai-api: ä½¿ç”¨OpenAIå…¼å®¹APIï¼ˆç™¾ç‚¼ï¼‰")
        print("  --use-cpu: ä½¿ç”¨CPUè¿è¡Œæ¨¡å‹ï¼ˆä»…å¯¹Ollamaæœ‰æ•ˆï¼‰")
        sys.exit(1)
    
    json_path = sys.argv[1]
    root_folder = sys.argv[2]
    mode_name = sys.argv[3]
    mode_dir = sys.argv[4]
    
    # è®¾ç½®æ—¶é—´è®°å½•ç›®å½•
    time_log_dir = 'time/star/hd_llm'
    if len(sys.argv) > 5 and not sys.argv[5].startswith('--'):
        time_log_dir = sys.argv[5]
    
    # åˆ›å»ºæ—¶é—´è®°å½•ç›®å½•
    os.makedirs(time_log_dir, exist_ok=True)
    
    # Check for flags
    no_think = '--no-think' in sys.argv
    use_openai_api = '--use-openai-api' in sys.argv
    use_cpu = '--use-cpu' in sys.argv
    
    # ç”Ÿæˆæ—¶é—´è®°å½•æ–‡ä»¶åï¼ˆåŸºäºæ¨¡å‹åç§°å’Œæ¨¡å¼ï¼‰
    model_safe_name = mode_name.replace(':', '_').replace('/', '_')
    think_suffix = '_nothink' if no_think else ''
    api_suffix = '_openai' if use_openai_api else ''
    cpu_suffix = '_cpu' if use_cpu else ''
    time_log_file = os.path.join(time_log_dir, f'hd_llm_repair_{model_safe_name}{think_suffix}{api_suffix}{cpu_suffix}.json')
    
    print(f"é…ç½®ä¿¡æ¯:")
    print(f"  JSONè·¯å¾„: {json_path}")
    print(f"  æ ¹ç›®å½•: {root_folder}")
    print(f"  æ¨¡å‹: {mode_name}")
    print(f"  è¾“å‡ºç›®å½•: {mode_dir}")
    print(f"  æ—¶é—´è®°å½•: {time_log_file}")
    print(f"  æ— æ€è€ƒæ¨¡å¼: {no_think}")
    print(f"  OpenAI API: {use_openai_api}")
    print(f"  CPUæ¨¡å¼: {use_cpu}")
    
    # æ‰§è¡Œä¿®å¤
    repair_times = process_dockerfiles(json_path, root_folder, mode_name, mode_dir, time_log_file, no_think, use_openai_api, use_cpu)
    
    # ç§»é™¤æ³¨é‡Š
    # remove_comments_in_lines(mode_dir)
    
    # ç”Ÿæˆæ‘˜è¦æŠ¥å‘Š
    summary_file = os.path.join(time_log_dir, f'summary_hd_llm_repair_{model_safe_name}{think_suffix}{api_suffix}{cpu_suffix}.json')
    generate_summary_report(time_log_file, summary_file)
    
    print(f"\næ‰€æœ‰å¤„ç†å®Œæˆï¼æ—¶é—´è®°å½•ä¿å­˜åœ¨: {time_log_dir}")

if __name__ == "__main__":
    main()

    # python repair_methods/hd_LLM.py "evaluate_result/dataset_fast_star1000+_dockerfile.json" "dataset_fast/star1000+_dockerfile" "qwen3:32b" "repair_result/dataset_fast/star1000+_dockerfile/qwen3_32b_hd_LLM_nothink" --no-think
    # python repair_methods/hd_LLM.py "evaluate_result/dataset_fast_star1000+_dockerfile.json" "dataset_fast/star1000+_dockerfile" "qwen3:8b" "repair_result/dataset_fast/star1000+_dockerfile/qwen3_8b_hd_LLM_nothink" --no-think
# python repair_methods/hd_LLM.py "evaluate_result/dataset_fast_star1000+_dockerfile.json" "dataset_fast/star1000+_dockerfile" "qwen3-235b-a22b-instruct-2507" "repair_result/dataset_fast/star1000+_dockerfile/qwen3_235b_hd_LLM_1" --use-openai-api
# python repair_methods/hd_LLM.py "evaluate_result/dataset_fast_star1000+_dockerfile.json" "dataset_fast/star1000+_dockerfile" "qwen3:0.6b" "repair_result/dataset_fast/star1000+_dockerfile/qwen3_06b_hd_LLM_nothink" --no-think