import json
import requests
import random
import re
from tqdm import tqdm
import fnmatch
import os
import sys
import signal
import time
from datetime import datetime

def send_message_and_get_response(message, model_name, no_think=False, use_openai_api=False):
    """å‘é€æ¶ˆæ¯å¹¶è·å–å“åº”ï¼Œæ”¯æŒOpenAI APIå’Œæœ¬åœ°Ollama API"""
    if use_openai_api:
        return _call_openai_api(message, model_name)
    else:
        return _call_ollama_api(message, model_name, no_think)

def _call_openai_api(message, model_name):
    """è°ƒç”¨OpenAIå…¼å®¹APIï¼ˆç™¾ç‚¼ï¼‰"""
    try:
        from openai import OpenAI
        
        client = OpenAI(
            api_key="fake_api",
            base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
        )
        
        messages = [{"role": "user", "content": message}]
        
        # è®¾ç½®è¶…æ—¶
        def timeout_handler(signum, frame):
            raise TimeoutError("Request timed out")
        
        signal.signal(signal.SIGALRM, timeout_handler)
        signal.alarm(180)
        
        try:
            completion = client.chat.completions.create(
                model=model_name,
                messages=messages,
                stream=False,
                temperature=0.3,
                max_tokens=4096
            )
            signal.alarm(0)  # é‡ç½®è¶…æ—¶
            
            message_content = completion.choices[0].message.content
            
            # æå–Dockerfileå†…å®¹
            dockerfile_pattern = re.compile(r'```dockerfile(.*?)```', re.DOTALL | re.IGNORECASE)
            match = dockerfile_pattern.search(message_content)
            if match:
                dockerfile_content = match.group(1).strip()
                return dockerfile_content
            else:
                print("No Dockerfile found in the response")
                return None
                
        except TimeoutError:
            print("Request timed out after 180 seconds")
            return None
        except Exception as e:
            print(f"OpenAI APIè°ƒç”¨é”™è¯¯: {str(e)}")
            return None
            
    except ImportError:
        print("OpenAIåº“æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install openai")
        return None
    except Exception as e:
        print(f"åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯é”™è¯¯: {str(e)}")
        return None

def _call_ollama_api(message, model_name, no_think=False):
    """è°ƒç”¨æœ¬åœ°Ollama API"""
    url = "http://localhost:11434/api/chat"
    
    # æ¨¡å‹å·®å¼‚åŒ–æ§åˆ¶
    if no_think:
        if "qwen3" in model_name.lower():
            message = f"/no_think\n\n{message}"
    
    messages = [{"role": "user", "content": message}]
    
    payload = {
        "model": model_name,
        "messages": messages,
        "stream": False,
        "options": {
            "temperature": 0.3,
            "num_predict": 8192
        }
    }

    try:
        # è®¾ç½®è¶…æ—¶
        def timeout_handler(signum, frame):
            raise requests.exceptions.Timeout("Request timed out")

        signal.signal(signal.SIGALRM, timeout_handler)
        signal.alarm(180)

        response = requests.post(url, json=payload)
        signal.alarm(0)

        if response.status_code == 200:
            result = response.json()
            message_content = result['message']['content']
            
            # æå–Dockerfileå†…å®¹
            dockerfile_pattern = re.compile(r'```dockerfile(.*?)```', re.DOTALL | re.IGNORECASE)
            match = dockerfile_pattern.search(message_content)
            if match:
                dockerfile_content = match.group(1).strip()
                return dockerfile_content
            else:
                print("No Dockerfile found in the response")
                return None
        else:
            print(f"APIè¿”å›é”™è¯¯çŠ¶æ€ç : {response.status_code}")
            return None

    except requests.exceptions.Timeout:
        print("Request timed out after 180 seconds")
        return None
    except requests.exceptions.RequestException as e:
        print("Error:", str(e))
        return None
    except Exception as e:
        print("Unexpected error:", str(e))
        return None

def remove_comments_in_lines(folder_path):
    """ç§»é™¤Dockerfileä¸­çš„æ³¨é‡Š"""
    # éå†æŒ‡å®šæ–‡ä»¶å¤¹ä¸‹çš„æ‰€æœ‰æ–‡ä»¶
    for filename in os.listdir(folder_path):
        filepath = os.path.join(folder_path, filename)
        print(f"å¤„ç†æ–‡ä»¶: {filename}")
        
        # è¯»å–æ–‡ä»¶å†…å®¹
        with open(filepath, 'r') as f:
            lines = f.readlines()
            
        # å¤„ç†æ–‡ä»¶å†…å®¹ï¼Œå»é™¤æ¯è¡Œå†…çš„æ³¨é‡Š
        new_lines = []
        for line in lines:
            line = line.rstrip()
            comment_index = line.find('#')
            if comment_index != -1:
                line = line[:comment_index].rstrip()
            new_lines.append(line + '\n')
            
        # å°†å¤„ç†åçš„å†…å®¹å†™å›æ–‡ä»¶
        with open(filepath, 'w') as f:
            f.writelines(new_lines)
            
    print(f"å·²å®Œæˆ: {folder_path}")

def parse_log_content(log_content):
    """è§£æåŒ…å«<phase>ã€<path>å’Œ<error>æ ‡ç­¾çš„æ—¥å¿—å†…å®¹"""
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–å„éƒ¨åˆ†å†…å®¹
    phase_pattern = r'<phase>(.*?)<phase>'
    path_pattern = r'<path>(.*?)<path>'
    error_pattern = r'<error>(.*?)<error>'

    phase = re.search(phase_pattern, log_content)
    path = re.search(path_pattern, log_content)
    error = re.search(error_pattern, log_content)

    # æå–åŒ¹é…åˆ°çš„å†…å®¹
    phase_content = phase.group(1) if phase else None
    path_content = path.group(1) if path else None
    error_content = error.group(1) if error else None

    return {
        'phase': phase_content,
        'path': path_content,
        'error': error_content
    }

def extract_original_path(full_path):
    """æå–æ ¼å¼ï¼šdataset_fast/éšåçš„è·¯å¾„/æœ€åæ–‡ä»¶å"""
    parts = full_path.split('/')
    try:
        # æ‰¾åˆ°'dataset_fast'çš„ç´¢å¼•ä½ç½®
        start_idx = parts.index('dataset_fast')
        # è·å–å‰ä¸¤ä¸ªç›®å½•å’Œæœ€åæ–‡ä»¶å
        original_path = '/'.join([
            parts[start_idx],       # dataset_fast
            parts[start_idx+1],     # ctf_dockerfile
            parts[-1]               # æœ€åæ–‡ä»¶å
        ])
        return original_path
    except (ValueError, IndexError):
        return None

def process_dockerfiles(unbuild_path, model_name="qwen3:32b", no_think=False, use_openai_api=False):
    """å¤„ç†Dockerfilesä¿®å¤ï¼Œæ”¯æŒOpenAI API"""
    
    with open(unbuild_path, 'r', encoding='utf-8') as file:
        unbuild_content = file.readlines()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = "build_repair_result"
    
    os.makedirs(output_dir, exist_ok=True)
    
    # è®°å½•å¤„ç†ç»“æœ
    repair_records = []
    
    for line in tqdm(unbuild_content, desc="ä¿®å¤æ„å»ºå¤±è´¥çš„Dockerfiles"):
        line = line.strip()
        if not line:
            continue
            
        parsed_log = parse_log_content(line)
        original_path = extract_original_path(parsed_log['path'])
        repair_path = parsed_log['path']
        
        if not original_path or not os.path.exists(original_path):
            print(f"åŸå§‹æ–‡ä»¶ä¸å­˜åœ¨: {original_path}")
            continue
            
        if not os.path.exists(repair_path):
            print(f"ä¿®å¤æ–‡ä»¶ä¸å­˜åœ¨: {repair_path}")
            continue
        
        # è¯»å–æ–‡ä»¶å†…å®¹
        with open(original_path, 'r', encoding='utf-8') as file:
            original_content = file.read()
        with open(repair_path, 'r', encoding='utf-8') as file:
            repair_content = file.read()
        
        # ç¡®å®šå¤±è´¥é˜¶æ®µ
        last_step = parsed_log['phase'] if parsed_log['phase'] else "beginning"
        
        # ç”Ÿæˆè¾“å‡ºè·¯å¾„
        relative_path = os.path.relpath(repair_path, "repair_result")
        modified_filepath = os.path.join(output_dir, relative_path)
        
        # æ£€æŸ¥ç›®æ ‡æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
        if os.path.exists(modified_filepath):
            print(f"æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡å¤„ç†: {modified_filepath}")
            repair_records.append({
                'original_path': original_path,
                'repair_path': repair_path,
                'output_path': modified_filepath,
                'status': 'skipped',
                'reason': 'already_exists',
                'timestamp': datetime.now().isoformat()
            })
            continue

        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(os.path.dirname(modified_filepath), exist_ok=True)
        
        # æ„é€ prompt
        prompt = (
            f"## Dockerfile Repair Analysis\n"
            f"**Original Dockerfile**:\n```dockerfile\n{original_content}\n```\n\n"
            f"**Repaired Dockerfile**:\n```dockerfile\n{repair_content}\n```\n"
            f"**Build Error**: `{parsed_log['error']}` (Failed at: {last_step})\n\n"
            "## Requirements\n"
            "Generate a corrected Dockerfile that:\n"
            "1. Retains ALL original functionality\n"
            "2. Fixes the build error while preserving docker smell repairs\n"
            "3. NO unrelated changes or new features\n"
            "4. Format:\n```dockerfile\n...\n```"
        )
        
        # è°ƒç”¨LLMè¿›è¡Œä¿®å¤
        start_time = time.time()
        modified_content = send_message_and_get_response(prompt, model_name, no_think, use_openai_api)
        repair_time = time.time() - start_time
        
        # ä¿å­˜ç»“æœ
        if modified_content:
            with open(modified_filepath, 'w', encoding='utf-8') as file:
                file.write(modified_content)
            
            status = 'success'
            reason = 'LLM repair successful'
            print(f"âœ… ä¿®å¤æˆåŠŸ: {repair_path} -> {modified_filepath} ({repair_time:.2f}s)")
        else:
            # å¦‚æœLLMä¿®å¤å¤±è´¥ï¼Œä¿å­˜åŸå§‹ä¿®å¤å†…å®¹
            with open(modified_filepath, 'w', encoding='utf-8') as file:
                file.write(repair_content)
            
            status = 'failed'
            reason = 'LLM repair failed, saved original repair'
            print(f"âŒ ä¿®å¤å¤±è´¥: {repair_path} -> {modified_filepath} ({repair_time:.2f}s)")
        
        # è®°å½•å¤„ç†ç»“æœ
        repair_records.append({
            'original_path': original_path,
            'repair_path': repair_path,
            'output_path': modified_filepath,
            'status': status,
            'reason': reason,
            'repair_time_seconds': round(repair_time, 2),
            'build_error': parsed_log['error'],
            'failed_at': last_step,
            'model': model_name,
            'no_think': no_think,
            'api_type': 'openai' if use_openai_api else 'ollama',
            'timestamp': datetime.now().isoformat()
        })
    
    # ä¿å­˜å¤„ç†è®°å½•
    if repair_records:
        record_file = f"build_repair_records_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(record_file, 'w', encoding='utf-8') as f:
            json.dump(repair_records, f, indent=2, ensure_ascii=False)
        print(f"ğŸ“Š ä¿®å¤è®°å½•å·²ä¿å­˜: {record_file}")
    
    # ç”Ÿæˆç»Ÿè®¡æ‘˜è¦
    generate_repair_summary(repair_records, output_dir)
    
    # ç§»é™¤æ³¨é‡Š
    # remove_comments_in_lines(os.path.dirname(modified_filepath))
    
    print("æ‰€æœ‰Dockerfileså¤„ç†å®Œæˆã€‚")

def generate_repair_summary(repair_records, output_dir):
    """ç”Ÿæˆä¿®å¤ç»Ÿè®¡æ‘˜è¦"""
    if not repair_records:
        return
    
    successful_repairs = [r for r in repair_records if r['status'] == 'success']
    failed_repairs = [r for r in repair_records if r['status'] == 'failed']
    skipped_repairs = [r for r in repair_records if r['status'] == 'skipped']
    
    summary = {
        'total_files': len(repair_records),
        'successful_repairs': len(successful_repairs),
        'failed_repairs': len(failed_repairs),
        'skipped_repairs': len(skipped_repairs),
        'success_rate': round(len(successful_repairs) / len(repair_records) * 100, 2) if repair_records else 0,
        'avg_repair_time': round(sum(r.get('repair_time_seconds', 0) for r in repair_records) / len(repair_records), 2) if repair_records else 0,
        'total_processing_time': round(sum(r.get('repair_time_seconds', 0) for r in repair_records), 2),
        'output_directory': output_dir,
        'timestamp': datetime.now().isoformat()
    }
    
    # æ‰“å°æ‘˜è¦
    print(f"\nğŸ“Š ä¿®å¤ç»Ÿè®¡æ‘˜è¦:")
    print(f"   æ€»å¤„ç†æ–‡ä»¶æ•°: {summary['total_files']}")
    print(f"   æˆåŠŸä¿®å¤: {summary['successful_repairs']} ({summary['success_rate']}%)")
    print(f"   ä¿®å¤å¤±è´¥: {summary['failed_repairs']}")
    print(f"   è·³è¿‡ä¿®å¤: {summary['skipped_repairs']}")
    print(f"   å¹³å‡ä¿®å¤æ—¶é—´: {summary['avg_repair_time']}ç§’")
    print(f"   æ€»å¤„ç†æ—¶é—´: {summary['total_processing_time']}ç§’")
    print(f"   è¾“å‡ºç›®å½•: {output_dir}")
    
    # ä¿å­˜æ‘˜è¦
    summary_file = os.path.join(output_dir, "repair_summary.json")
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(summary, f, indent=2, ensure_ascii=False)
    print(f"ğŸ“„ ç»Ÿè®¡æ‘˜è¦å·²ä¿å­˜: {summary_file}")

def main():
    if len(sys.argv) < 2:
        print("Usage: python build_repair.py unbuild_path [model_name] [--no-think] [--use-openai-api]")
        print("\nå‚æ•°è¯´æ˜:")
        print("  unbuild_path: åŒ…å«æ„å»ºå¤±è´¥æ—¥å¿—çš„æ–‡ä»¶è·¯å¾„")
        print("  model_name: æ¨¡å‹åç§° (é»˜è®¤: qwen3:32b)")
        print("  --no-think: å¯ç”¨æ— æ€è€ƒæ¨¡å¼ï¼ˆä»…å¯¹Qwenæœ‰æ•ˆï¼‰")
        print("  --use-openai-api: ä½¿ç”¨OpenAIå…¼å®¹APIï¼ˆç™¾ç‚¼ï¼‰")
        sys.exit(1)
    
    unbuild_path = sys.argv[1]
    
    # é»˜è®¤æ¨¡å‹åç§°
    # model_name = "qwen3:32b"
    no_think = False
    use_openai_api = False
    
    # è§£æå‚æ•°
    for i in range(2, len(sys.argv)):
        if sys.argv[i] == "--no-think":
            no_think = True
        elif sys.argv[i] == "--use-openai-api":
            use_openai_api = True
        elif not sys.argv[i].startswith("--"):
            model_name = sys.argv[i]
    
    print(f"ğŸ”§ é…ç½®ä¿¡æ¯:")
    print(f"  æ„å»ºå¤±è´¥æ—¥å¿—: {unbuild_path}")
    print(f"  æ¨¡å‹åç§°: {model_name}")
    print(f"  æ— æ€è€ƒæ¨¡å¼: {no_think}")
    print(f"  OpenAI API: {use_openai_api}")
    
    # æ‰§è¡Œä¿®å¤
    process_dockerfiles(unbuild_path, model_name, no_think, use_openai_api)

if __name__ == "__main__":
    main()


# python repair_methods/build_repair.py evaluate_result/star/qwen3_235b_hd_LLM/_unbuild.txt "qwen3-235b-a22b-instruct-2507" --use-openai-api